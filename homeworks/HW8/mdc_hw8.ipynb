{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 8: Q-Learning\n",
    "\n",
    "In this homework, we will be implementing Q-learning for the Pacman environment.\n",
    "The inspiration for Pacman trials comes from medical RL research using simulated rat mazes in [*Learning to use working memory: a reinforcement learning gating model of rule acquisition in rats*](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3483721/) and [*Memory Alone Does Not Account for the Way Rats Learn a Simple Spatial Alternation Task*](https://www.jneurosci.org/content/40/38/7311/)\n",
    "\n",
    "As usual, sections where you have to write code are denoted by a `#CODE HERE`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Environment\n",
    "\n",
    "We're using an adapted version of the Pacman environment developed right here by Berkeley professors for CS188!\n",
    "Although we will only code in qLearningAgents.py, feel free to explore the environment for more maps, documented implementations, and ways to test our agents. Relevant python files:\n",
    "\n",
    "| File | Description |\n",
    "| :--- | :-- |\n",
    "| `qLearningAgents.py` | Our Q-learning code, used in `pacman` and `gridworld`. All coding tasks are in this file |\n",
    "| `pacman.py` | Runs Pacman games |\n",
    "| `layouts/` | Various Pacman maps |\n",
    "| `gridworld.py` | Simplified simulation from demo, has different maps |\n",
    "| `featureExtractor.py` | Functions for converting Pacman game state to a few observables |\n",
    "| `utils.py`  | Useful data structures, probabilistic methods, and misc functions |\n",
    "\n",
    "You should have numpy and torch from installed from previous assignments.\n",
    "\n",
    "### Running Code\n",
    "In a terminal, cd into the folder then run any of these commands. `python pacman.py -h` and `python gridworld.py -h` will explain what each argument does -- feel free to change them up!\n",
    "\n",
    "The demo I ran in class, using your implementation (good for stepping through Q updates when debugging):\n",
    "- `python gridworld.py --agent q -g BridgeGrid --manual --noise 0 --learningRate 1.0 --discount 0.9 --episodes 100`\n",
    "\n",
    "Training and visualizing Q-learning for Pacman:\n",
    "- `python pacman.py -numTraining 100 -numGames 110 --layout smallGrid -p PacmanQAgent`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Q-learning (tabular)\n",
    "Read through `QLearningAgent` in `qLearningAgents.py`. Fill in these functions:\n",
    "\n",
    "### Policy:\n",
    "Implement a policy based on the Q values, accessed with `self.getQValue(state,action)`. The best action should maximize our estimated discounted rewards -- the Q value. \n",
    "- Fill in `getBestAction`\n",
    "\n",
    "### Exploration vs Exploitation:\n",
    "During training, we want to find new strategies through random exploration. \n",
    "- Fill in `getAction`\n",
    "\n",
    "### Bellman Update:\n",
    "Update the Q function towards the Bellman equation. We want our Q to match the recursive property of the ideal Q function.\n",
    "\n",
    "$$\\text{target} = r_t + \\gamma \\max_{a_{t+1}} Q(s_{t+1},a_{t+1})$$\n",
    "- Fill in `update`\n",
    "\n",
    "$$Q(s_t, a_t) \\leftarrow (1-\\alpha)Q(s_t, a_t) + \\alpha * \\text{target}$$\n",
    "- Fill in `incrementQValue`\n",
    "\n",
    "### Run It!\n",
    "\n",
    "Test it out on `gridworld.py` and see if you're getting Q values you expect.\n",
    "- `python gridworld.py --agent q -g BridgeGrid --manual --noise 0 --learningRate 0.5 --discount 0.9 --episodes 100`\n",
    "\n",
    "Once that works, it's time for the real test: `pacman.py`. Let's see how your agent fares against ghosts!\n",
    "- `python pacman.py -numTraining 500 -numGames 505 --layout smallGrid -p PacmanQAgent`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Deep Q-learning\n",
    "\n",
    "### Using Features\n",
    "\n",
    "In reality, rats don't know their x,y coordinates, they can only sense their local environment. Since we shouldn't have access to the state of the world, replace $Q(s_t, a_t)$ with $Q(f_t)$, where $f_t$ is a set of sensible features defined in `featureExtractor.py`:\n",
    "- Are we about to hit a food (can be seen)\n",
    "- How far is the closest food (can be smelled!)\n",
    "- Are we adjacent to a ghost (you can feel the spookiness!)\n",
    "\n",
    "Read through `featurizedQAgent`, notice how we only had to modify accesses to self.Q_values, so the rest of the functions are unchanged!\n",
    "\n",
    "Test out our featurizedQAgent on `pacman.py`.\n",
    "- `python pacman.py -numTraining 500 -numGames 505 --layout smallGrid -p FeaturizedQAgent`\n",
    "\n",
    "### Using Neural Nets\n",
    "\n",
    "Instead of storing a value of Q for each possible $(s_t, a_t)$, or now each possible $f_t$, let's fit a function! Much like line fitting, once a parameterized function is fit to some datapoints, it can generalize to never before seen data!\n",
    "\n",
    "Our Bellman Update becomes\n",
    "$$\\text{target} = r_t + \\gamma \\max_{a_{t+1}} Q_\\theta(s_{t+1},a_{t+1})$$\n",
    "$$\\text{loss} = \\left(Q_\\theta(s_t, a_t) - \\text{target}\\right)^2$$\n",
    "$$\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta \\text{loss}$$\n",
    "\n",
    "The only difference is how we increment our Q values!\n",
    "\n",
    "Read through `DeepQAgent`. This 1-layer neural network is implemented exactly like you programmed COVID models in Assignment 2! Notice we are using MSE \"Mean Squared Error\" loss.\n",
    "\n",
    "Test out our deepQAgent on `pacman.py`. Instead of learning a separate value for each possible combination of features, hopefully it should learn faster by generalizing!\n",
    "- `python pacman.py -numTraining 300 -numGames 305 --layout smallGrid -p DeepQAgent`\n",
    "\n",
    "Now let's see it on a real map!\n",
    "- `python pacman.py -numTraining 300 -numGames 305 --layout mediumClassic -p DeepQAgent`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Submission\n",
    "\n",
    "Nice job implementing Q-learning! Please upload `qlearningAgents.py` [here](https://forms.gle/AMWsG57KxHU3Zd7v7), and feel free to include any cool pacman videos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
