{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R46-rwd4hNMn"
   },
   "source": [
    "# Week 4 Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eiiwjw4yhX0P"
   },
   "source": [
    "This demo is created for **MLAB's meDecal**. It is inspired by the manuscript, **A Primer on Deep Learning in Genomics** (*Nature Genetics, 2018*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Libraries\n",
    "As usual, we will first import the tools we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fb-0kBFAts0-"
   },
   "source": [
    "## 0. Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QABdxctitugX"
   },
   "source": [
    "In this demo, we will use CNN to approach an important problem in functional genomics: **the discovery of transcription-factor binding sites in DNA**.\n",
    "\n",
    "But wait.. what is transcription-factor binding sites? check out this [deck](https://docs.google.com/presentation/d/1FpBBlRK-rN4BDHJslq1kbJ_t0TILcY9RRiZvv5RTjmI/edit?usp=sharing) for more background on the Biology.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "99NcA3rnyTCH"
   },
   "source": [
    "As we go through this notebook, we will  design a neural network that can discover binding motifs in DNA based on the results of an assay that determines whether a longer DNA sequence binds to the transcription factor (protein) or not. Here, the longer DNA sequences are our *independent variables* (or *predictors*), while the positive or negative response of the assay is the *dependent variable* (or *response*).\n",
    "\n",
    "We will use simulated data that consists of DNA sequences of length 50 bases (chosen to be artificially short so that the data is easy to play around with), and is labeled with 0 or 1 depending on the result of the assay. Our goal is to build a classifier that can predict whether a particular sequence will bind to the protein.\n",
    "\n",
    "(Spoiler alert: the true regulatory motif is *`CGACCGAACTCC`*. Of course, the neural network doesn't know this.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aK7wr8n8gzQ_"
   },
   "source": [
    "## 1. Curate the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5T-FgQrNq1vS"
   },
   "source": [
    "![alt text](https://github.com/athenaleong/Medecal_Misc/blob/main/step_1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QRMSFdUSubgX"
   },
   "source": [
    "In order to train the neural network, we must load and preprocess the data, which consists of DNA sequences and their corresponding labels.By processing this data, the network will learn to distinguish sequences that bind to the transcription factor from those that do not. \n",
    "\n",
    "We start by loading the simulated data from an external repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "B_F7VoAMhLiX",
    "outputId": "13550a3d-1dde-471c-ddb2-ac352e2c5df5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "\n",
    "SEQUENCES_URL = 'https://raw.githubusercontent.com/abidlabs/deep-learning-genomics-primer/master/sequences.txt'\n",
    "\n",
    "sequences = requests.get(SEQUENCES_URL).text.split('\\n')\n",
    "sequences = list(filter(None, sequences))  # This removes empty sequences.\n",
    "\n",
    "# Let's print the first few sequences.\n",
    "pd.DataFrame(sequences, index=np.arange(1, len(sequences)+1), \n",
    "             columns=['Sequences']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bzsbNHqWiFek"
   },
   "source": [
    "The next  step is to organize the data into a format that can be passed into a deep learning algorithm. Most deep learning algorithms accept data in the form of vectors or matrices (or more generally, tensors). \n",
    "\n",
    "To get each DNA sequence in the form of a matrix, we use _one-hot encoding_, which encodes every base in a sequence in the form of a 4-dimensional vector, with a separate dimension for each base. We place a \"1\" in the dimension corresponding to the base found in the DNA sequence, and \"0\"s in all other slots. We then concatenate these 4-dimensional vectors together along the bases in the sequence to form a matrix. \n",
    "\n",
    "In the cell below, we one-hot encode the simulated DNA sequences, and show an example of what the one-hot encoded sequence looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "IPJD6PuDnaS6",
    "outputId": "587f6674-4b4b-482c-ca76-86ac153ad09f"
   },
   "outputs": [],
   "source": [
    "# The LabelEncoder encodes a sequence of bases as a sequence of integers.\n",
    "integer_encoder = LabelEncoder()  \n",
    "# The OneHotEncoder converts an array of integers to a sparse matrix where \n",
    "# each row corresponds to one possible value of each feature.\n",
    "one_hot_encoder = OneHotEncoder(categories='auto')   \n",
    "input_features = []\n",
    "\n",
    "for sequence in sequences:\n",
    "  integer_encoded = integer_encoder.fit_transform(list(sequence))\n",
    "  integer_encoded = np.array(integer_encoded).reshape(-1, 1)\n",
    "  one_hot_encoded = one_hot_encoder.fit_transform(integer_encoded)\n",
    "  input_features.append(one_hot_encoded.T.toarray())\n",
    "\n",
    "np.set_printoptions(threshold=40)\n",
    "input_features = np.stack(input_features)\n",
    "print(\"Example sequence\\n-----------------------\")\n",
    "print('DNA Sequence #1:\\n',sequences[0][:10],'...',sequences[0][-10:])\n",
    "print('One hot encoding of Sequence #1:\\n',input_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AbBmrEVGrahN"
   },
   "source": [
    "Similarly, we can go ahead and load the labels. In this case, the labels are structured as follows: a \"1\" indicates that a protein bound to the sequence, while a \"0\" indicates that the protein did not. While we could use the labels as a vector, it is often easier to similarly one-hot encode the labels, as we did the features. We carry out that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "IA9FJeQkr1Ze",
    "outputId": "7c8825c7-7a85-498f-a285-df5036401050"
   },
   "outputs": [],
   "source": [
    "LABELS_URL = 'https://raw.githubusercontent.com/abidlabs/deep-learning-genomics-primer/master/labels.txt'\n",
    "\n",
    "labels = requests.get(LABELS_URL).text.split('\\n')\n",
    "labels = list(filter(None, labels))  # removes empty sequences\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(categories='auto')\n",
    "labels = np.array(labels).reshape(-1, 1)\n",
    "input_labels = one_hot_encoder.fit_transform(labels).toarray()\n",
    "\n",
    "print('Labels:\\n',labels.T)\n",
    "print('One-hot encoded labels:\\n',input_labels.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MTokFzDZvQR-"
   },
   "source": [
    "Next, We will go ahead and split the data into three different sub-datasets:\n",
    "\n",
    "(1) Training dataset: a dataset used to fit the parameters of a model or to define the weights of connections between neurons of a neural network.\n",
    "\n",
    "(2) Validation dataset: a second dataset used to minimize overfitting. The weights of the network are not adjusted with this data set. After each training cycle, if the accuracy over the training data set increases, but the accuracy over the validation data set stays the same or decreases, then there is overfitting on the neural network.\n",
    "\n",
    "(3) Testing dataset: is a third dataset not included in the training nor validation data sets. After all the training and validation cycles are complete, this dataset is used only for testing the final solution in order to measure the actual predictive power of the neural network on new examples.\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_7LKgvc3Lnn"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    input_features, input_labels, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25)\n",
    "\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_train = torch.from_numpy(X_train)\n",
    "y_train = y_train.astype(int)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "\n",
    "X_val = X_val.astype(np.float32)\n",
    "X_val = torch.from_numpy(X_val)\n",
    "y_val = y_val.astype(int)\n",
    "y_val = torch.from_numpy(y_val)\n",
    "\n",
    "X_test = X_test.astype(np.float32)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y_test = y_test.astype(int)\n",
    "y_test = torch.from_numpy(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7LQp2ZFrg6dm"
   },
   "source": [
    "## 2. Select the Architecture and Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xBT6Q3j-sjhh"
   },
   "source": [
    "![alt text](https://github.com/athenaleong/Medecal_Misc/blob/main/step_2.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "krHJgtK_rzif"
   },
   "source": [
    "Next, we choose a neural network architecture to train the model. In this tutorial, we choose a simple 1D convolutional neural network (CNN), which is commonly used in deep learning for functional genomics applications.\n",
    "\n",
    "A CNN learns to recognize patterns that are generally invariant across space, by trying to match the input sequence to a number of learnable \"filters\" of a fixed size. In our dataset, the filters will be motifs within the DNA sequences. The CNN may then learn to combine these filters to recognize a larger structure (e.g. the presence or absence of a transcription factor binding site). \n",
    "\n",
    "To construct the CNN, We will use the deep learning library `Pytorch`. \n",
    "\n",
    "- _Conv1D_: We define our convolutional layer to have 32 filters of size 12 bases.\n",
    "\n",
    "- _MaxPooling_: After the convolution, we use a pooling layer to down-sample the output of the each of the 32 convolutional filters. Though not always required, this is a typical form of non-linear down-sampling used in CNNs.\n",
    "\n",
    "- _Flatten_: This layer flattens the output of the max pooling layer, combining the results of the convolution and pooling layers across all 32 filters. \n",
    "\n",
    "- _Dense_ / _Linear_: The first Dense tensor creates a layer (dense_1) that compresses the representation of the flattened layer, resulting in smaller layer with 16 tensors, and the second Dense function converges the tensors into the output layer (dense_2) that consists of the two possible response values (0 or 1).\n",
    "\n",
    "We can see the details of the architecture of the neural network we have created by running `print(model)`, which prints the dimensionality and number of parameters for each layer in our network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        #Set up layers\n",
    "        self.conv1 = nn.Sequential(nn.Conv1d(4, 32, kernel_size=12),\n",
    "                    nn.MaxPool1d(4),\n",
    "                    nn.Flatten())\n",
    "        self.fc1 = nn.Sequential(nn.Flatten(), nn.Linear(288, 16),\n",
    "                                nn.ReLU())\n",
    "        self.fc2 = nn.Sequential(nn.Linear(16,2), \n",
    "                                nn.Softmax())\n",
    "        \n",
    "        #Initialize optimizer\n",
    "        self.optimizer = Adam(self.parameters())\n",
    "        \n",
    "        #Initialize loss function\n",
    "        self.loss = nn.BCELoss()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.conv1(X)\n",
    "        X = self.fc1(X)\n",
    "        output = self.fc2(X)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check out model summary\n",
    "model = ConvNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(prediction, target):\n",
    "    \"\"\"\n",
    "    Helper function to calculate accuracy\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    prediction = list(prediction.detach().numpy())\n",
    "    for t, p in zip(prediction, target):\n",
    "        \n",
    "        if np.argmax(t) == np.argmax(p):\n",
    "            correct += 1\n",
    "    \n",
    "    return correct / len(target)\n",
    "    \n",
    "\n",
    "def train_and_display(model, epoch):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        \n",
    "        #Compute training\n",
    "        pred_train = model(X_train)\n",
    "        pred_val = model(X_val)\n",
    "        \n",
    "        #Compute validation loss\n",
    "        loss_train = model.loss(pred_train, y_train.float())\n",
    "        loss_val = model.loss(pred_val, y_val.float())\n",
    "        \n",
    "        #Compute accuracy\n",
    "        train_accuracy = accuracy(pred_train, y_train)\n",
    "        val_accuracy = accuracy(pred_val, y_val)\n",
    "        \n",
    "        #Zero parameter gradient & Optimize loss\n",
    "        model.optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        model.optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss_train)\n",
    "        val_losses.append(loss_val)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        #Logging at regular intervals\n",
    "        if e % 5 == 0 or e == epoch - 1:\n",
    "            print('Epoch : ',e+1, '\\t', 'loss :', loss_val, '\\t', 'accuracy:', val_accuracy)\n",
    "            \n",
    "            \n",
    "    #Plot training metrics over time \n",
    "    f, (plt1, plt2) = plt.subplots(2)\n",
    "    plt1.plot(train_losses, label='Training loss')\n",
    "    plt1.plot(val_losses, label='Validation loss')\n",
    "    plt1.set_title('model loss')\n",
    "    plt1.set_ylabel('loss')\n",
    "    plt1.set_xlabel('epoch')\n",
    "    plt1.legend()\n",
    "    plt2.plot(train_accuracies, label='Training accuracy')\n",
    "    plt2.plot(val_accuracies, label='Validation accuracy')\n",
    "    plt2.set_title('model accuracy')\n",
    "    plt2.set_ylabel('accuracy')\n",
    "    plt2.set_xlabel('epoch')\n",
    "    plt2.legend()\n",
    "    \n",
    "    f.subplots_adjust(hspace=0.5)\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "train_and_display(model, 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P5eKDmX8ODBE"
   },
   "source": [
    "Similarly, we can plot the accuracy of our neural network on the binary classification task. The metric used in this example is the _binary accuracy_, which calculates the proportion of predictions that match labels or response variables. Other metrics may be used in different tasks -- for example, the _mean squared error_ is typically used to measure the accuracy for continuous response variables (e.g. polygenic risk scores, total serum cholesterol level, height, weight and systolic blood pressure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Xy7VhhZg-hN"
   },
   "source": [
    "## 3. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "btf7FyMVsnFA"
   },
   "source": [
    "![alt text](https://github.com/athenaleong/Medecal_Misc/blob/main/step_3.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eQ_xYCvfvFlE"
   },
   "source": [
    "The best way to evaluate whether the network has learned to classify sequences is to evaluate its performance on a fresh test set consisting of data that it has not observed at all during training. Here, we evaluate the model on the test set and plot the results as a confusion matrix. Nearly every test sequence should be correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "y_predicted = model(X_test)\n",
    "y_predicted = list(y_predicted.detach().numpy())\n",
    "\n",
    "cm = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_predicted, axis=1))\n",
    "\n",
    "#Visualisation of confusion matrix\n",
    "print('Confusion matrix:\\n',cm)\n",
    "cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
    "plt.imshow(cm, cmap=plt.cm.Blues)\n",
    "plt.title('Normalized confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.xlabel('True label')\n",
    "plt.ylabel('Predicted label')\n",
    "plt.xticks([0, 1]); plt.yticks([0, 1])\n",
    "plt.grid(False)\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], '.2f'),\n",
    "             horizontalalignment='center',\n",
    "             color='white' if cm[i, j] > 0.5 else 'black')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week_4 Demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
